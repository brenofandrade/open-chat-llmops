services:
  app:
    build: .
    container_name: open-chat-app
    depends_on:
      - mlflow
    environment:
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MLFLOW_EXPERIMENT_NAME: open-chat-llmops
      OLLAMA_BASE_URL: http://host.docker.internal:11434
      OLLAMA_MODEL: llama3.2
      OLLAMA_TEMPERATURE: "0.2"
      OLLAMA_TOP_P: "0.9"
      PORT: "8000"
    ports:
      - "8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"

  ui:
    build: .
    container_name: open-chat-ui
    depends_on:
      - app
    command: streamlit run src/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
    environment:
      BACKEND_URL: http://app:8000
    ports:
      - "8501:8501"

  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.8.1
    container_name: open-chat-mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --allowed-hosts mlflow,mlflow:5000,open-chat-mlflow,open-chat-mlflow:5000,localhost,localhost:5000,127.0.0.1,127.0.0.1:5000
      --backend-store-uri sqlite:////mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow

volumes:
  mlflow_data:
