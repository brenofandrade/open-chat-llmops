# open-chat-llmops
Local LLM chat & MLflow Observability.

A robust implementation of a local chatbot ecosystem designed with **LLMOps** principles. This project bridges the gap between running local open-source models and professional production monitoring.

By combining **Ollama** for seamless model serving and **MLflow** for deep observability, you can track every interaction, compare model performances, and optimize prompts in a fully private environment.

---

## üõ†Ô∏è Technology Stack

* **[Ollama](https://ollama.com/):** The engine for running high-performance open-source models (Llama 3, Mistral, Phi-3) locally. It handles model management and provides a clean API for inference.
* **[MLflow](https://mlflow.org/):** An open-source platform for the machine learning lifecycle. In this project, it is used for **LLM Tracking & Tracing**:
    * Logging inputs, outputs, and system prompts.
    * Monitoring latency and execution time.
    * Versioning different model configurations (Temperature, Top-P).
* **[Python](https://www.python.org/):** The core logic orchestrating the communication between the LLM and the tracking server.

---

## üöÄ Getting Started





---

üìú License
Distributed under the MIT License. See LICENSE for more information.


--- Built by Breno F. Andrade
